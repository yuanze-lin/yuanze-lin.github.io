<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yuanze Lin</title>

    <meta name="author" content="Yuanze Lin">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/oxford.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yuanze Lin
                </p>
                <p>I am a Computer Science DPhil student at <a href="https://www.ox.ac.uk/">University of Oxford</a>, advised by <a href="https://scholar.google.com/citations?hl=en&user=0bzqo0YAAAAJ">Ronald Clark</a> and <a href="https://scholar.google.com/citations?hl=en&user=185g9ckAAAAJ">Niki Trigoni</a>, my research interests include self-supervised learning, multimodal learning and large language models.
                <p>
                  Before going to Oxford, I've done research at <a href="https://www.microsoft.com/en-us/research/project/project-florence-vl/publications/">Microsoft Redmond</a>, <a href="https://www.msra.cn/">MSRA</a>, <a href="https://www.jhu.edu/">Johns Hopkins University</a>, <a href="https://www.tsinghua.edu.cn/en/"> Tsinghua University</a>, etc. I'm so lucky to work with <a href="https://www.microsoft.com/en-us/research/people/xunguo/"> Xun Guo</a> and 
                  <a href="https://www.microsoft.com/en-us/research/people/yanlu/"> Yan Lu</a> at MSRA,  <a href="http://www.gaohuang.net/"> Gao Huang</a> at Tsinghua University, <a href="https://sites.google.com/view/yujia"> Yujia Xie</a>, <a href="https://www.dongdongchen.bid/"> Dongdong Chen</a> and <a href="https://xycking.wixsite.com/yichongxu">Yichong Xu</a> at Microsoft Redmond, <a href="https://cihangxie.github.io/"> Cihang Xie</a> and <a href="https://www.cs.jhu.edu/~ayuille/"> Alan Yuille</a> at Johns Hopkins University.
                </p>
                <p style="text-align:center">
                  <a href="mailto:yuanze.lin@cs.ox.ac.uk">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=0WFC2w0AAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://openreview.net/profile?id=~Yuanze_Lin1">OpenReview</a> &nbsp;/&nbsp;
                  <a href="https://github.com/yuanze-lin/">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/yuanze-lin-720543139/">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/yuanze.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/yuanze_circle.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm particularly interested in computer vision, especially about how to efficiently utilize images and texts for pre-training or solving various multimodal tasks.  Selected papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
    
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/smaug.png' width="160">
        </td>
        
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2211.11446">
            <span class="papertitle">SMAUG: Sparse Masked Autoencoder for Efficient Video-Language Pre-training</span>
          </a>
          <br>
          <strong>Yuanze Lin</strong>,
          <a href="https://weichen582.github.io/">Chen Wei</a>,
          <a href="https://csrhddlam.github.io/">Huiyu Wang</a>,
          <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
          <a href="https://cihangxie.github.io/">Cihang Xie</a>
          <br>
          <em>ICCV</em>, 2023 
          <br>
          <a href="https://arxiv.org/pdf/2211.11446">ArXiv</a>
          /
          <a href="images/poster_smaug.pdf">Poster</a>
          /
          <a href="https://arxiv.org/pdf/2211.11446">Video</a>
          <p></p>
          <p>
          Propose an efficient multimodal pre-training framework, which enjoys both competitive performances on text-to-video retrieval and video question answering tasks, and much less pre-training costs by 1.9X or more.
          </p>
        </td>
      </tr>
      
      
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/revive.png' width="160">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/44956951349095f74492a5471128a7e0-Paper-Conference.pdf">
            <span class="papertitle">REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering</span>
          </a>
          <br>
  <strong>Yuanze Lin</strong>, <a href="https://sites.google.com/view/yujia">Yujia Xie</a>, <a href="https://www.dongdongchen.bid/">Dongdong Chen</a>, <a href="https://xycking.wixsite.com/yichongxu">Yichong Xu</a>, <a href="https://scholar.google.com/citations?user=1b2kKWoAAAAJ&hl=zh-CN">Chenguang Zhu</a>, 
  <a href="https://scholar.google.com/citations?user=k9TsUVsAAAAJ&hl=zh-CN">Lu Yuan</a>
          
          <br>
          <em>NeurIPS</em>, 2022
          <br>
          <a href="https://arxiv.org/abs/2206.01201">ArXiv</a> / 
          <a href="https://neurips.cc/media/PosterPDFs/NeurIPS%202022/0bf727e907c5fc9d5356f11e4c45d613.png?t=1667706317.0158653">Poster</a> /
          <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/44956951349095f74492a5471128a7e0-Supplemental-Conference.pdf">Supplementary Material</a> /
          <a href="https://openreview.net/forum?id=wwyiEyK-G5D">OpenReview</a> /
          <a href="https://github.com/yuanze-lin/REVIVE">Code</a> 
          <p></p>
          <p>Propose a new knowledge-based VQA method REVIVE, which tries to utilize the explicit information of object regions not only in the knowledge retrieval stage but also in the answering model. We perform extensive experiments on the standard OK-VQA dataset and achieve new state-of-the-art performance.</p>
        </td>
      </tr>

      

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/pseudoq.png' width="160">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_Pseudo-Q_Generating_Pseudo_Language_Queries_for_Visual_Grounding_CVPR_2022_paper.pdf">
            <span class="papertitle">Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding</span>
          </a>
          <br>
          <a href="https://github.com/jianghaojun">Haojun Jiang*</a>,
          <strong>Yuanze Lin*</strong>,
          <a href="https://scholar.google.com/citations?user=wv3U3tkAAAAJ&hl=en">Dongchen Han,</a>,
          <a href="https://scholar.google.com/citations?user=rw6vWdcAAAAJ&hl=zh-CN">Shiji Song</a>,  
          <a href="http://www.gaohuang.net/">Gao Huang</a>
          <br>
          <em>CVPR</em>, 2022
          <br>
          <a href="https://arxiv.org/abs/2203.08481">ArXiv</a> /
          <a href="https://cloud.tsinghua.edu.cn/f/e5f6df930e5d4b21ae27/">Poster</a> /
          <a href="https://cloud.tsinghua.edu.cn/f/d655d6e2a6b246b4bb4f/">Video</a> /
          <a href="https://github.com/LeapLabTHU/Pseudo-Q">Code</a> 
          <p></p>
          <p>
          Present a novel method, named Pseudo-Q, to automatically generate pseudo language queries for supervised training, t achieves superior or comparable performance compared to state-of-the-art weakly-supervised visual grounding methods on all the five datasets we have experimented.
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/adafocus2.png' width="160">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_AdaFocus_V2_End-to-End_Training_of_Spatial_Dynamic_Networks_for_Video_CVPR_2022_paper.pdf">
            <span class="papertitle">AdaFocus V2: End-to-End Training of Spatial Dynamic Networks for Video Recognition</span>
          </a>
          <br>
          <a href="https://scholar.google.com/citations?user=gBP38gcAAAAJ&hl=zh-CN">Yulin Wang*</a>,
          <a href="https://scholar.google.com/citations?user=Q9cLkdcAAAAJ&hl=en">Yang Yue*</a>,
          <strong>Yuanze Lin</strong>,
          <a href="https://github.com/jianghaojun">Haojun Jiang</a>,
          <a href="https://scholar.google.com/citations?user=31eXgMYAAAAJ&hl=en">Zihang Lai</a>,
          <a href="https://scholar.google.com/citations?hl=en&user=9WmuhUcAAAAJ">Victor Kulikov</a>,
          <a href="https://scholar.google.com/citations?hl=en&user=_V4gCO4AAAAJ">Nikita Orlov</a>,
          <a href="https://scholar.google.com/citations?hl=en&user=WBvt5A8AAAAJ">Humphrey Shi</a>,
          <a href="http://www.gaohuang.net/">Gao Huang</a>
          <br>
          <em>CVPR</em>, 2022
          <br>
          <a href="https://arxiv.org/abs/2112.14238">ArXiv</a> /
          <a href="https://github.com/LeapLabTHU/AdaFocusV2">Code</a> 
          <p></p>
          <p>
          Reformulate the training of AdaFocus as a simple one-stage algorithm by introducing a differentiable interpolation-based patch selection operation and further present an improved training scheme. Extensive experiments on six benchmark datasets demonstrate its effectiveness.
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/mcn.png' width="160">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Lin_Self-Supervised_Video_Representation_Learning_With_Meta-Contrastive_Network_ICCV_2021_paper.pdf">
            <span class="papertitle">Self-supervised video representation learning with meta-contrastive network</span>
          </a>
          <br>
          <strong>Yuanze Lin</strong>,
          <a href="https://scholar.google.com/citations?user=Ow4R8-EAAAAJ&hl=en">Xun Guo</a>,
          <a href="https://scholar.google.com/citations?user=djk5l-4AAAAJ&hl=en">Yan Lu</a>
          <br>
          <em>ICCV</em>, 2021
          <br>
          <a href="https://arxiv.org/abs/2108.08426">ArXiv</a> /
          <a href="https://arxiv.org/abs/2108.08426">Poster</a> 
          <p></p>
          <p>
            Propose a Meta-Contrastive Network (MCN), which combines the contrastive learning and meta learning, to enhance the learning ability of existing self-supervised approaches, For two downstream tasks, i.e., video action recognition and video retrieval, MCN outperforms state-of-the-art approaches on UCF101 and HMDB51 datasets. 
          </p>
        </td>
      </tr>

            
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/eva_gcn.png' width="160">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/CVPR2021W/AMFG/papers/Xin_EVA-GCN_Head_Pose_Estimation_Based_on_Graph_Convolutional_Networks_CVPRW_2021_paper.pdf">
            <span class="papertitle">EVA-GCN: Head Pose Estimation Based on Graph Convolutional Networks</span>
          </a>
          <br>
          <a href="https://scholar.google.com/citations?hl=zh-CN&user=gZfZZVQAAAAJ">Miao Xin</a>,
          <a href="https://scholar.google.com/citations?hl=zh-CN&user=6aYncPAAAAAJ">Shentong Mo</a>,
          <strong>Yuanze Lin</strong>
          <br>
          <em>CVPR AMFG Workshop</em>, 2021 &nbsp <font color="red"><strong>(Best Paper Award)</strong></font>
          <br>
          <a href="https://openaccess.thecvf.com/content/CVPR2021W/AMFG/papers/Xin_EVA-GCN_Head_Pose_Estimation_Based_on_Graph_Convolutional_Networks_CVPRW_2021_paper.pdf">Paper</a> /
          <a href="https://github.com/stoneMo/EVA-GCN">Code</a> 
          <p></p>
          <p>
            Construct a landmark-connection graph, and propose to leverage the Graph Convolutional Networks (GCN) to model the complex nonlinear mappings between the graph typologies and the head pose angles.
          </p>
        </td>
      </tr>
            
          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Experience</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <tr>
              <td>
                <li>Research Assistant, Johns Hopkins University (CCVL), May.2022 - Feb.2023</li>
                <br>
                <li>Research Assistant, University of California, Merced, May.2022 - Feb.2023</li>
                <br>
                <li>Researcher Intern, Microsoft Research Redmond, Feb.2022 - June.2022</li>
                <br>
                <li>Research Assistant, Tsinghua University, Sep.2021 - Mar.2022</li>
                <br>
                <li>Researcher Intern, Microsoft Research Asia, Dec.2020 - Sep.2021</li>
                <br>
                <li>Research Assistant, Institute of Automation, CAS, Nov.2019 - Mar.2020</li>
              </td>
            </tr>

            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Last Update: 10/2023  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  <a href="https://jonbarron.info/">Template</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
