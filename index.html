                  <!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title style="font-family:Comic Sans MS; font-size:48px">Yuanze Lin</title>

    <meta name="author" content="Yuanze Lin">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/oxford1.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align:center;font-family:Comic Sans MS;font-size:32px">
                  Yuanze Lin
                </p>
                <p>I am a first-year DPhil student in the Computer Science department at <a href="https://www.ox.ac.uk/">University of Oxford</a>, where I work with Prof. <a href="https://www.cs.ox.ac.uk/people/ronald.clark/">Ronald Clark</a>, focusing on text-to-3d and vision-language models.
                <p>
                  Before going to Oxford, I spent great time at <a href="https://www.microsoft.com/en-us/research/project/project-florence-vl/publications/">Microsoft Redmond</a>, <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">MSRA</a>, <a href="https://ccvl.jhu.edu/">CCVL @ Johns Hopkins University</a>, <a href="https://www.tsinghua.edu.cn/en/"> Tsinghua University</a>, etc. I'm so lucky to work with Dr. <a href="https://www.microsoft.com/en-us/research/people/xunguo/"> Xun Guo</a> and 
                  Dr. <a href="https://www.microsoft.com/en-us/research/people/yanlu/"> Yan Lu</a> at MSRA, Prof. <a href="http://www.gaohuang.net/"> Gao Huang</a> at Tsinghua University, Dr. <a href="https://sites.google.com/view/yujia"> Yujia Xie</a>, Dr. <a href="https://www.dongdongchen.bid/"> Dongdong Chen</a> and Dr. <a href="https://xycking.wixsite.com/yichongxu"> Yichong Xu</a> at Microsoft Redmond, Prof. <a href="https://cihangxie.github.io/"> Cihang Xie</a> and Prof. <a href="https://www.cs.jhu.edu/~ayuille/"> Alan Yuille</a> at Johns Hopkins University,
		     Dr. <a href="https://sites.google.com/site/yihsuantsai/"> Yi-Hsuan Tsai</a> and Prof. <a href="https://faculty.ucmerced.edu/mhyang/"> Ming-Hsuan Yang</a> at UC Merced.
                </p>
                <p>
                  My research interest lies in computer vision and machine learning, with an emphasis on self-supervised learning, multimodal learning, and large language models. Feel free to email me for research discussion : )
                </p>
                <p>
                 <font color='#ff0000'><em> Currently, I'm open to 2024 winter and summer research internships, especially about text-to-3d and vision-language models. </em></font>
                </p>
                <p style="text-align:center">
                  <a href="mailto:yuanze.lin@cs.ox.ac.uk">yuanze.lin [at] cs.ox.ac.uk</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=0WFC2w0AAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/yuanze-lin/">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/yuanze-lin-720543139/">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/YuanzeL.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/YuanzeL_circle.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2 style="font-family:Impact; font-size:24px; font-weight:bold"><img src="icons/news.png", style="width:32px;height:auto" /> &nbsp; News</h2>
		<hr />
		<p style="line-height:0.8em;"> [11/2023]&nbsp;&nbsp; A new work <a href="https://yuanze-lin.me/LearnableRegions_page/">Text-Driven Image Editing via Learnable Regions</a> has been released. </p>
                <p style="line-height:0.8em;"> [10/2023]&nbsp;&nbsp; Started my PhD journey at CS @ University of Oxford. </p>
                <p style="line-height:0.8em;"> [07/2023]&nbsp;&nbsp; SMAUG accepted to ICCV 2023. </p>
                <p style="line-height:0.8em;"> [09/2022]&nbsp;&nbsp; REVIVE accepted to NeurIPS 2022. </p>
                <p style="line-height:0.8em;"> [03/2022]&nbsp;&nbsp; Pseudo-Q and AdaFocus V2 accepted to CVPR 2022. </p>
                <p style="line-height:0.8em;"> [07/2021]&nbsp;&nbsp; MCN accepted to ICCV 2021. </p>
                <p style="line-height:0.8em;"> [06/2021]&nbsp;&nbsp; EVA-GCN accepted to CVPR 2021 AMFG Workshop and won Best Paper Award. </p>
              </td>
            <tr>
          </tbody></table>
	  
            
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2 style="font-family:Impact; font-size:24px; font-weight:bold"><img src="icons/research.png", style="width:32px;height:auto;" /> &nbsp;Research</h2>
		<hr />
                <p>
                  I'm particularly interested in computer vision, especially about how to efficiently utilize images and texts for pre-training or solving various multimodal tasks. Papers are sorted by recency.
                </p>
		<p>
                  (* denotes equal contribution)
		</p>
              </td>
            </tr>
          </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/learnable_regions.png' width="160" height="130">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2311.16432.pdf">
            <span class="papertitle">Text-Driven Image Editing via Learnable Regions</span>
          </a>
          <br>
          <strong>Yuanze Lin</strong>,
          <a href="https://wenz116.github.io/">Yi-Wen Chen</a>,
          <a href="https://sites.google.com/site/yihsuantsai/">Yi-Hsuan Tsai</a>,
          <a href="http://www.lujiang.info/">Lu Jiang</a>,
          <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>
          <br>
          <em>ArXiv</em>, 2023 
          <br>
          <a href="https://arxiv.org/abs/2311.16432">ArXiv</a>
          /
          <a href="https://yuanze-lin.me/LearnableRegions_page/">Project Page</a>
	  /
	  <a href="https://www.youtube.com/watch?v=FpMWRXFraK8">Video</a> 
	  /
	  <a href="https://github.com/yuanze-lin/Learnable_Regions">Code</a> 
	  /
	  <a href="bibtex/lin2023text.txt">BibTex</a>
          <p></p>
          <p>
          Introduce a region-based editing network that is trained to generate editing regions utilizing a text-driven editing loss with CLIP guidance, our method can edit the given images based on freely provided language descriptions.
          </p>
        </td>
      </tr>
		
        <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/smaug.png' width="160" height="130">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_SMAUG_Sparse_Masked_Autoencoder_for_Efficient_Video-Language_Pre-Training_ICCV_2023_paper.pdf">
            <span class="papertitle">SMAUG: Sparse Masked Autoencoder for Efficient Video-Language Pre-training</span>
          </a>
          <br>
          <strong>Yuanze Lin</strong>,
          <a href="https://weichen582.github.io/">Chen Wei</a>,
          <a href="https://csrhddlam.github.io/">Huiyu Wang</a>,
          <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
          <a href="https://cihangxie.github.io/">Cihang Xie</a>
          <br>
          <em>ICCV</em>, 2023 
          <br>
          <a href="https://arxiv.org/pdf/2211.11446">ArXiv</a>
          /
          <a href="images/poster_smaug.pdf">Poster</a>
          /
          <a href="images/Presentation_iccv2023.pptx">Slides</a>
          /
          <a href="bibtex/lin2023smaug.txt">BibTex</a>
          <p></p>
          <p>
          Propose an efficient multimodal pre-training framework, which enjoys both competitive performances on text-to-video retrieval and video question answering tasks, and much less pre-training costs by 1.9X or more.
          </p>
        </td>
      </tr>
      
      
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/revive.png' width="160" height="130">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/44956951349095f74492a5471128a7e0-Paper-Conference.pdf">
            <span class="papertitle">REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering</span>
          </a>
          <br>
  <strong>Yuanze Lin</strong>, <a href="https://sites.google.com/view/yujia">Yujia Xie</a>, <a href="https://www.dongdongchen.bid/">Dongdong Chen</a>, <a href="https://xycking.wixsite.com/yichongxu">Yichong Xu</a>, <a href="https://scholar.google.com/citations?user=1b2kKWoAAAAJ&hl=zh-CN">Chenguang Zhu</a>, 
  <a href="https://scholar.google.com/citations?user=k9TsUVsAAAAJ&hl=zh-CN">Lu Yuan</a> 
          <br>
          <em>NeurIPS</em>, 2022
          <br>
          <a href="https://arxiv.org/abs/2206.01201">ArXiv</a> / 
          <a href="https://neurips.cc/media/PosterPDFs/NeurIPS%202022/0bf727e907c5fc9d5356f11e4c45d613.png?t=1667706317.0158653">Poster</a> /
          <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/44956951349095f74492a5471128a7e0-Supplemental-Conference.pdf">Supplementary Material</a> /
          <a href="https://openreview.net/forum?id=wwyiEyK-G5D">OpenReview</a> /
          <a href="https://github.com/yuanze-lin/REVIVE">Code</a> /
          <a href="bibtex/lin2022revive.txt">BibTeX</a>
          <p></p>
          <p>Propose a new knowledge-based VQA method REVIVE, which utilizes the explicit information of object regions not only in the knowledge retrieval stage but also in the answering model. It achieves a new state-of-the-art performance on OK-VQA dataset.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/pseudoq.png' width="160" height="130">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_Pseudo-Q_Generating_Pseudo_Language_Queries_for_Visual_Grounding_CVPR_2022_paper.pdf">
            <span class="papertitle">Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding</span>
          </a>
          <br>
	  <strong>Yuanze Lin*</strong>,
          <a href="https://github.com/jianghaojun">Haojun Jiang*</a>,
          <a href="https://scholar.google.com/citations?user=wv3U3tkAAAAJ&hl=en">Dongchen Han</a>,
          <a href="https://scholar.google.com/citations?user=rw6vWdcAAAAJ&hl=zh-CN">Shiji Song</a>,  
          <a href="http://www.gaohuang.net/">Gao Huang</a>
          <br>
          <em>CVPR</em>, 2022
          <br>
          <a href="https://arxiv.org/abs/2203.08481">ArXiv</a> /
          <a href="https://cloud.tsinghua.edu.cn/f/e5f6df930e5d4b21ae27/">Poster</a> /
          <a href="https://github.com/LeapLabTHU/Pseudo-Q">Code</a> /
          <a href="bibtex/jiang2022pseudo.txt">BibTeX</a>
          <p></p>
          <p>
          Present Pseudo-Q to automatically generate pseudo language queries for supervised training, which achieves superior or comparable performance compared to existing weakly-supervised visual grounding methods on five datasets.
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/adafocus2.png' width="160" height="130">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_AdaFocus_V2_End-to-End_Training_of_Spatial_Dynamic_Networks_for_Video_CVPR_2022_paper.pdf">
            <span class="papertitle">AdaFocus V2: End-to-End Training of Spatial Dynamic Networks for Video Recognition</span>
          </a>
          <br>
          <a href="https://scholar.google.com/citations?user=gBP38gcAAAAJ&hl=zh-CN">Yulin Wang*</a>,
          <a href="https://scholar.google.com/citations?user=Q9cLkdcAAAAJ&hl=en">Yang Yue*</a>,
          <strong>Yuanze Lin</strong>,
          <a href="https://github.com/jianghaojun">Haojun Jiang</a>,
          <a href="https://scholar.google.com/citations?user=31eXgMYAAAAJ&hl=en">Zihang Lai</a>,
          <a href="https://scholar.google.com/citations?hl=en&user=9WmuhUcAAAAJ">Victor Kulikov</a>,
          <a href="https://scholar.google.com/citations?hl=en&user=_V4gCO4AAAAJ">Nikita Orlov</a>,
          <a href="https://scholar.google.com/citations?hl=en&user=WBvt5A8AAAAJ">Humphrey Shi</a>,
          <a href="http://www.gaohuang.net/">Gao Huang</a>
          <br>
          <em>CVPR</em>, 2022
          <br>
          <a href="https://arxiv.org/abs/2112.14238">ArXiv</a> /
          <a href="https://github.com/LeapLabTHU/AdaFocusV2">Code</a> /
          <a href="bibtex/wang2022adafocus.txt">BibTeX</a>
          <p></p>
          <p>
          Reformulate AdaFocus as a simple one-stage algorithm by introducing a differentiable interpolation-based patch selection operation and further present an improved training scheme. Extensive experiments on six benchmark datasets demonstrate its effectiveness.
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/mcn.png' width="160" height="130">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Lin_Self-Supervised_Video_Representation_Learning_With_Meta-Contrastive_Network_ICCV_2021_paper.pdf">
            <span class="papertitle">Self-supervised video representation learning with meta-contrastive network</span>
          </a>
          <br>
          <strong>Yuanze Lin</strong>,
          <a href="https://scholar.google.com/citations?user=Ow4R8-EAAAAJ&hl=en">Xun Guo</a>,
          <a href="https://scholar.google.com/citations?user=djk5l-4AAAAJ&hl=en">Yan Lu</a>
          <br>
          <em>ICCV</em>, 2021
          <br>
          <a href="https://arxiv.org/abs/2108.08426">ArXiv</a> /
          <a href="https://arxiv.org/abs/2108.08426">Poster</a> /
          <a href="bibtex/lin2021self.txt">BibTeX</a>
          <p></p>
          <p>
            Propose a Meta-Contrastive Network (MCN), which combines contrastive learning and meta learning. For video action recognition and video retrieval tasks, MCN outperforms state-of-the-art approaches on UCF101 and HMDB51 datasets. 
          </p>
        </td>
      </tr>
            
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/eva_gcn.png' width="160" height="130">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/CVPR2021W/AMFG/papers/Xin_EVA-GCN_Head_Pose_Estimation_Based_on_Graph_Convolutional_Networks_CVPRW_2021_paper.pdf">
            <span class="papertitle">EVA-GCN: Head Pose Estimation Based on Graph Convolutional Networks</span>
          </a>
          <br>
          <a href="https://scholar.google.com/citations?hl=zh-CN&user=gZfZZVQAAAAJ">Miao Xin</a>,
          <a href="https://scholar.google.com/citations?hl=zh-CN&user=6aYncPAAAAAJ">Shentong Mo</a>,
          <strong>Yuanze Lin</strong>
          <br>
          <em>CVPR AMFG Workshop</em>, 2021 &nbsp <a href="images/workshop_best_paper.jpg"><font color="red"><strong>(Best Paper Award)</strong></font></a> 
          <br>
          <a href="https://openaccess.thecvf.com/content/CVPR2021W/AMFG/papers/Xin_EVA-GCN_Head_Pose_Estimation_Based_on_Graph_Convolutional_Networks_CVPRW_2021_paper.pdf">Paper</a> /
          <a href="https://github.com/stoneMo/EVA-GCN">Code</a> /
          <a href="bibtex/xin2021eva.txt">BibTeX</a>
          <p></p>
          <p>
            Construct a landmark-connection graph, and propose to leverage the Graph Convolutional Networks (GCN) to model the complex nonlinear mappings between the graph typologies and the head pose angles.
          </p>
        </td>
      </tr>        
      </tbody></table>

	
      <table style="width:100%;border:0px;padding-top:20px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding-left:20px;padding-right:20px;width:100%;vertical-align:middle">
        <h2 style="font-family:Impact; font-size:24px; font-weight:bold"><img src="icons/experience2.png", style="width:32px;height:auto;" /> &nbsp;Experiences</h2>
	<hr />
	</td>
	</tr>    
	</tbody></table>

	<table style="width:100%;border:0px;border-spacing:0px;;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr> <td style="padding-left:23px;width:15%;vertical-align:center;"> 
	<img src='icons/uc_merced.png' style="width:85px;height:auto;" /> 
	</td> <td style="padding-left:0px;width:85%;vertical-align:left;line-height:150%;text-align:left;"> 
        <a href="https://www.ucmerced.edu/"> <span class="papertitle">UC Merced</span> 
	</a> <br> <strong>Visiting Student, May 2022 - Nov 2023</strong> <br>
        <em>Supervised by Dr. <a href="https://scholar.google.com/citations?user=zjI51wEAAAAJ&hl=en"> Yi-Hsuan Tsai</a> and Prof. <a href="https://scholar.google.com/citations?user=p9-ohHsAAAAJ&hl=en"> Ming-Hsuan Yang</a>, work on text-driven image editing</em> 
	</td></tr>
		
        <tr> <td style="padding-left:15px;width:15%;vertical-align:center;"> 
	<img src='icons/alibaba2.png' style="width:115px;height:auto;" /> 
	</td> <td style="padding:0px;width:85%;vertical-align:left;line-height:150%;text-align:left;"> 
        <a href="https://www.alibabagroup.com/en-US/"> <span class="papertitle">Alibaba Group</span> 
	</a> <br> <strong>Senior Algorithm Engineer, Feb 2023 - Aug 2023</strong> <br>
        <em>Work on multimodal pre-training/fine-tuning and large language models (LLMs) </em> 
	</td></tr>

        <tr> <td style="padding-left:31px;width:15%;vertical-align:center;"> 
	<img src='icons/jhu2.png' style="width:70px;height:auto;" /> 
	</td> <td style="padding:0px;width:85%;vertical-align:left;line-height:150%;text-align:left;"> 
        <a href="https://ccvl.jhu.edu/"> <span class="papertitle">Johns Hopkins University (CCVL)</span> 
	</a> <br> <strong>Research Assistant, May 2022 - Feb 2023</strong> <br>
        <em>Supervised by Prof. <a href="https://scholar.google.com/citations?user=X3vVZPcAAAAJ&hl=en"> Cihang Xie</a> and Prof. <a href="https://scholar.google.com/citations?user=FJ-huxgAAAAJ&hl=en"> Alan Yuille</a>, work on efficient multimodal pre-training based on MAE</em> 
	</td></tr>

	<tr> <td style="padding-left:11px;width:15%;vertical-align:center;"> <img src='icons/microsoft2.jpg' style="width:110px;height:auto;" /> 
	</td> <td style="padding:0px;width:85%;vertical-align:left;line-height:150%;text-align:left;"> 
	<a href="https://www.microsoft.com/en-us/research/project/project-florence-vl/publications/"> 
	<span class="papertitle">Microsoft Redmond</span> </a> <br> <strong>Researcher Intern, Feb 2022 - June 2022</strong> 
	<br> <em>Supervised by Dr. <a href="https://scholar.google.com/citations?user=r2FiAE4AAAAJ&hl=en"> Yujia Xie</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=sYKpKqEAAAAJ"> Dongdong Chen</a> and Dr. <a href="https://scholar.google.com/citations?user=sYza2XwAAAAJ&hl=en"> Yichong Xu</a>, work on knowledge-based VQA</em> 
	</td> </tr> 


	<tr> <td style="padding-left:25px;width:15%;vertical-align:center;"> <img src='icons/tsinghua.png' style="width:80px;height:auto;" /> 
	</td> <td style="padding:0px;width:85%;vertical-align:left;line-height:150%;text-align:left;"> 
	<a href="https://www.gaohuang.net/"> 
	<span class="papertitle">Tsinghua University</span> </a> <br> <strong>Research Assistant, Sep 2021 - Mar 2022</strong> 
	<br> <em>Supervised by Prof. <a href="https://scholar.google.com/citations?user=-P9LwcgAAAAJ&hl=en"> Gao Huang</a>, work on visual grounding and efficient video recognition</em> 
	</td> </tr> 

	<tr> <td style="padding-left:14px;width:15%;vertical-align:center;border:30px;"> <img src='icons/Microsoft_Research_Asia_logo.png' style="object-fit:scale-down;width:100px;height:120px" /> 
	</td> <td style="padding:0px;width:85%;vertical-align:left;line-height:150%;text-align:left;"> 
	<a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/"> 
	<span class="papertitle">Microsoft Research Asia</span> </a> <br> <strong>Researcher Intern, Dec 2020 - Sep 2021</strong> 
	<br> <em>Supervised by Dr. <a href="https://scholar.google.com/citations?user=Ow4R8-EAAAAJ&hl=en"> Xun Guo</a> and Dr. <a href="https://scholar.google.com/citations?user=djk5l-4AAAAJ&hl=en"> Yan Lu</a>, work on self-supervised learning and transformers for video tasks</em> 
	</td> </tr> 
		

	<tr> <td style="padding-left:26px;width:15%;vertical-align:center;"> <img src='icons/tencent_ai2.png' style="width:80px;height:auto;" /> 
	</td> <td style="padding:0px;width:85%;vertical-align:left;line-height:150%;text-align:left;"> 
	<a href="https://ai.tencent.com/ailab/en/about"> 
	<span class="papertitle">Tencent AI Lab</span> </a> <br> <strong>Researcher Intern, Sep 2020 - Dec 2020</strong> 
	<br> <em>Supervised by Dr. <a href="https://scholar.google.com/citations?hl=en&user=wTJ83eEAAAAJ"> Haozhi Huang</a>, work on text-based editing of videos based on meta learning</em> 
	</td> </tr>

	</td>
	</tr>    
	</tbody></table>

<!--          <table style="width:100%;padding-top:70px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2 style="font-family:Impact; font-size:24px; font-weight:bold"><img src="icons/award2.png", style="width:32px;height:auto;" /> &nbsp; Selected Honors and Awards</h2>
		<hr />
                <p style="font-size:20px"> <li> Stars of Tomorrow (Microsoft Research Asia) </li> </p>
		<p style="line-height:0.4em;font-size:20px"> <li> CVPR AMFG 2021 Best Paper Award </li> </p>
		<p style="line-height:0.4em;font-size:20px"> <li> Beihang University Excellent Graduate </li> </p>
              </td>
            </tr>
          </tbody></table> -->
		
         <table style="width:100%;padding-top:30px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2 style="font-family:Impact; font-size:24px; font-weight:bold"><img src="icons/activity.png", style="width:32px;height:auto;" /> &nbsp; Services</h2>
		<hr />
                <p style="line-height:0.8em;font-size:14px;"> <em> Conference Reviewer: ICRA 2024, CVPR 2024 </em> </p>
		<p style="line-height:0.8em;font-size:14px;"> <em> Conference Reviewer: ICLR 2023, CVPR 2023, ICCV 2023, NeurIPS 2023 </em> </p>
		<p style="line-height:0.8em;font-size:14px;"> <em> Conference Reviewer: CVPR 2022 </em> </p>
              </td>
            </tr>
          </tbody></table>
	  
<!--          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
	      <td style="padding:20px;width:100%;vertical-align:middle">
		<a href="https://clustrmaps.com/site/1bwxo" title="Visit tracker"><img src="//clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=tt&d=LpeUFK6E0UqKyMayHxJ376iYqiMie7Njj0rKOZhMZhE" /></a>
              </td>
            </tr>
          </tbody></table> -->
	  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px">
                <br>
                <p style="text-align:right;font-size:small;">
                  No web trackers, feel free to see this website.  Last Update: 12/2023  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  <a href="https://jonbarron.info/">Template</a>
                </p>
              </td>
            </tr>
          </tbody></table>
          
  </body>
</html>
