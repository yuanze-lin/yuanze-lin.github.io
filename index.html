                  <!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title style="font-family:Comic Sans MS; font-size:48px">Yuanze Lin</title>

    <meta name="author" content="Yuanze Lin">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/oxford1.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align:center;font-family:Comic Sans MS;font-size:32px">
                  Yuanze Lin
                </p> 
                <p>I am a PhD student in the Computer Science Department at the <a href="https://www.ox.ac.uk/">University of Oxford</a>, where I work with Prof. Ronald Clark and Prof. Philip Torr. My research focuses on diffusion and vision-language models.
<!--                 <p>I am a PhD student in the Computer Science Department at the <a href="https://www.ox.ac.uk/">University of Oxford</a>, where I collaborate with Prof. <a href="https://scholar.google.com/citations?hl=en&user=kPxa2w0AAAAJ"> Philip Torr</a> and Dr. <a href="https://www.robots.ox.ac.uk/~joao/"> JoÃ£o F. Henriques</a>. My research focuses on diffusion and vision-language models. -->
                <p>
<!--                   Before going to Oxford, I spent great time at <a href="https://www.microsoft.com/en-us/research/project/project-florence-vl/publications/">Microsoft Redmond</a>, <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">MSR Asia</a>, <a href="https://ccvl.jhu.edu/">CCVL @ Johns Hopkins University</a>, <a href="https://www.alibabagroup.com/en-US/"> Alibaba</a>, etc. It's very fortunate to work with Dr. <a href="https://www.microsoft.com/en-us/research/people/xunguo/"> Xun Guo</a> and 
                  Dr. <a href="https://www.microsoft.com/en-us/research/people/yanlu/"> Yan Lu</a> at MSRA, Prof. <a href="http://www.gaohuang.net/"> Gao Huang</a> at Tsinghua University, Dr. <a href="https://sites.google.com/view/yujia"> Yujia Xie</a>, Dr. <a href="https://www.dongdongchen.bid/"> Dongdong Chen</a> and Dr. <a href="https://xycking.wixsite.com/yichongxu"> Yichong Xu</a> at Microsoft Redmond, Prof. <a href="https://cihangxie.github.io/"> Cihang Xie</a> and Prof. <a href="https://www.cs.jhu.edu/~ayuille/"> Alan Yuille</a> at Johns Hopkins University,
		  Dr. <a href="https://sites.google.com/site/yihsuantsai/"> Yi-Hsuan Tsai</a> and Prof. <a href="https://faculty.ucmerced.edu/mhyang/"> Ming-Hsuan Yang</a> at UC Merced. -->
			
                  Before going to Oxford, I had a great time at <a href="https://www.microsoft.com/en-us/research/project/project-florence-vl/publications/">Microsoft Redmond</a>, <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">MSR Asia</a>, <a href="https://ccvl.jhu.edu/">CCVL @ Johns Hopkins University</a>, <a href="https://www.alibabagroup.com/en-US/"> Alibaba</a>, etc. I appreciate collaborating with distinguished professors and researchers from these institutions.
                </p>
                <p>
                  My research interest lies in machine learning and its applications, especially:
		   <p style="line-height:0.6em;"> &nbsp;&#x2022;&nbsp;&nbsp; Multimodal large language models (MLLMs) </p>
		   <p style="line-height:0.6em;"> &nbsp;&#x2022;&nbsp;&nbsp; Image/Video generation and editing based on diffusion models </p>
		   <p style="line-height:0.6em;"> &nbsp;&#x2022;&nbsp;&nbsp; The applicability of large language models (LLMs) </p>
<!-- 		   <p style="line-height:0.6em;"> &nbsp;&#x2022;&nbsp;&nbsp; Self-supervised representation learning </p> -->
                </p>
                <p style="text-align:center">
                  <a href="mailto:yuanze.lin@cs.ox.ac.uk">yuanze.lin [at] cs.ox.ac.uk</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=0WFC2w0AAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/yuanze-lin/">GitHub</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/yuanze-lin-720543139/">LinkedIn</a>
                </p>
<!-- 	        <p style="color:red">  <b> I'm actively seeking research intern positions, feel free to contact me </b> ðŸ˜Š </p> -->
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%;">
                <a href="images/Yuanze.jpg"><img style="width:100%;max-width:100%;" alt="profile photo" src="images/Yuanze.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2 style="font-family:Impact; font-size:24px; font-weight:bold"><img src="icons/news.png", style="width:32px;height:auto" /> &nbsp; News</h2>
		<hr />
		<p style="line-height:0.8em;"> [06/2025]&nbsp;&nbsp; We introduced <a href="https://arxiv.org/pdf/2506.03150"> IllumiCraft</a> for high-fidelity video relighting.</font></p>
		<p style="line-height:0.8em;"> [04/2025]&nbsp;&nbsp; <span style="color:red;"> <a href="https://arxiv.org/pdf/2412.09612"> Olympus</a> was selected as a Highlight at CVPR 2025!</span></font></p>
		<p style="line-height:0.8em;"> [03/2025]&nbsp;&nbsp; Released the code of <a href="https://github.com/yuanze-lin/Olympus">Olympus (CVPR 2025)</a> for vision tasks. </font></p>
		<p style="line-height:0.8em;"> [02/2025]&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2412.09612"> Olympus</a> accepted to CVPR 2025.</span></font></p>
		<p style="line-height:0.8em;"> [12/2024]&nbsp;&nbsp; We presented <a href="https://arxiv.org/pdf/2412.09612"> Olympus</a> to solve over 20 different computer vision tasks.</font></p>
		<p style="line-height:0.8em;"> [08/2024]&nbsp;&nbsp; Released the code of <a href="https://github.com/yuanze-lin/Learnable_Regions">Learnable Regions (CVPR2024)</a> for image editing. </font></p>
		<p style="line-height:0.8em;"> [07/2024]&nbsp;&nbsp; <a href="https://arxiv.org/pdf/2407.04681.pdf"> Rethinking Visual Prompting for MLLMs with External Knowledge</a> was presented. </font></p>
		<p style="line-height:0.8em;"> [03/2024]&nbsp;&nbsp; Check out <a href="https://yuanze-lin.me/DreamPolisher_page/"> DreamPolisher</a> for high-quality text-to-3D generation! </font></p>
		<p style="line-height:0.8em;"> [02/2024]&nbsp;&nbsp; Started a research internship at <a href="https://www.microsoft.com/en-us/research/"> GenAI @ Microsoft</a>. </font></p>
		<p style="line-height:0.8em;"> [02/2024]&nbsp;&nbsp; <a href="https://yuanze-lin.me/LearnableRegions_page/"> Text-Driven Image Editing via Learnable Regions</a> accepted to CVPR 2024.</p>
                <p style="line-height:0.8em;"> [10/2023]&nbsp;&nbsp; Started my PhD journey at <a href="https://www.cs.ox.ac.uk/"> CS @ University of Oxford</a>. </p>
                <p style="line-height:0.8em;"> [07/2023]&nbsp;&nbsp; <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_SMAUG_Sparse_Masked_Autoencoder_for_Efficient_Video-Language_Pre-Training_ICCV_2023_paper.pdf"> SMAUG</a> accepted to ICCV 2023. </p>
                <p style="line-height:0.8em;"> [09/2022]&nbsp;&nbsp; <a href="https://papers.nips.cc/paper_files/paper/2022/file/44956951349095f74492a5471128a7e0-Paper-Conference.pdf"> REVIVE</a> accepted to NeurIPS 2022. </p>
                <p style="line-height:0.8em;"> [03/2022]&nbsp;&nbsp; <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_Pseudo-Q_Generating_Pseudo_Language_Queries_for_Visual_Grounding_CVPR_2022_paper.pdf"> Pseudo-Q</a> and <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_AdaFocus_V2_End-to-End_Training_of_Spatial_Dynamic_Networks_for_Video_CVPR_2022_paper.pdf"> AdaFocus V2</a> accepted to CVPR 2022. </p>
                <p style="line-height:0.8em;"> [07/2021]&nbsp;&nbsp; <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Lin_Self-Supervised_Video_Representation_Learning_With_Meta-Contrastive_Network_ICCV_2021_paper.pdf"> MCN</a> accepted to ICCV 2021. </p>
                <p style="line-height:0.8em;"> [06/2021]&nbsp;&nbsp; <a href="https://openaccess.thecvf.com/content/CVPR2021W/AMFG/papers/Xin_EVA-GCN_Head_Pose_Estimation_Based_on_Graph_Convolutional_Networks_CVPRW_2021_paper.pdf"> EVA-GCN</a> accepted to CVPR 2021 AMFG Workshop and won <b> Best Paper Award! </b> </p>
              </td>
            <tr>
          </tbody></table>
	  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle;">
                <h2 style="font-family:Impact; font-size:24px; font-weight:bold"><img src="icons/research.png", style="width:32px;height:auto;" /> &nbsp;Selected Publications</h2>
		<hr />
                <p>
                 Papers are sorted by recency, * denotes equal contribution.
                </p>
            </tr>
          </tbody></table>

        <table style="width:100%;margin-top:-40px;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
	    <img src='images/IllumiCraft.png' width="160" height="130">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2506.03150.pdf">
            <span class="papertitle">IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation</span>
          </a>
          <br>
          <strong>Yuanze Lin</strong>,
          <a href="https://wenz116.github.io/">Yi-Wen Chen</a>,
          <a href="https://sites.google.com/site/yihsuantsai/">Yi-Hsuan Tsai</a>,
          <a href="https://www.ron-clark.com/">Ronald Clark</a>,
          <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>
          <br>
          <em>Arxiv</em>, 2025&nbsp;</span> 
          <br>
          <a href="https://arxiv.org/pdf/2412.09612">ArXiv</a>
	  /
          <a href="https://yuanze-lin.me/IllumiCraft_page/">Project Page</a>
	  /
	  <a href="https://www.youtube.com/watch?v=qAV58sADEzo">Video</a> 
	  /
          <a href="https://github.com/yuanze-lin/IllumiCraft">Code</a>
	  /
	  <a href="bibtex/">BibTeX</a>
          <p></p>
          <p>
          We present IllumiCraft, a unified framework that unifies geometry and illumination diffusion for controllable video generation.
          </p>
        </td>
      </tr>
		
        <table style="width:100%;margin-top:-40px;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
	    <img src='images/Olympus.png' width="160" height="130">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2412.09612.pdf">
            <span class="papertitle">Olympus: A Universal Task Router for Computer Vision Tasks</span>
          </a>
          <br>
          <strong>Yuanze Lin</strong>,
          <a href="https://scholar.google.com/citations?user=hJrIyCwAAAAJ&hl=en">Yunsheng Li</a>,
          <a href="https://www.dongdongchen.bid/">Dongdong Chen</a>,
          <a href="https://weijianxu.com/">Weijian Xu</a>,
          <a href="https://www.ron-clark.com/">Ronald Clark</a>,
          <a href="https://eng.ox.ac.uk/people/philip-torr/">Philip H.S. Torr</a>
          <br>
          <em>CVPR</em>, 2025&nbsp; <span style="color: red;"><strong>(Highlight)</strong></span> 
          <br>
          <a href="https://arxiv.org/pdf/2412.09612">ArXiv</a>
	  /
          <a href="https://yuanze-lin.me/Olympus_page/">Project Page</a>
	  /
	  <a href="https://www.youtube.com/watch?v=N1xOdIrVvn4">Video</a> 
	  /
          <a href="https://github.com/yuanze-lin/Olympus">Code</a>
	  /
	  <a href="bibtex/lin2024olympus.txt">BibTeX</a>
          <p></p>
          <p>
          We introduce Olympus, a new approach that transforms Multimodal Large Language Models (MLLMs) into a unified framework capable of handling a wide array of computer vision tasks.
          </p>
        </td>
      </tr>
		
<!--         <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
	    <img src='images/rethinking.png' width="160" height="130">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2407.04681.pdf">
            <span class="papertitle">Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge</span>
          </a>
          <br>
          <strong>Yuanze Lin</strong>,
          <a href="https://scholar.google.com/citations?user=hJrIyCwAAAAJ&hl=en">Yunsheng Li</a>,
          <a href="https://www.dongdongchen.bid/">Dongdong Chen</a>,
          <a href="https://weijianxu.com/">Weijian Xu</a>,
          <a href="https://www.ron-clark.com/">Ronald Clark</a>,
          <a href="https://eng.ox.ac.uk/people/philip-torr/">Philip Torr</a>,
          <a href="https://www.microsoft.com/en-us/research/people/luyuan/">Lu Yuan</a>
          <br>
          <em>Preprint</em>, 2024
          <br>
          <a href="https://arxiv.org/pdf/2407.04681">ArXiv</a>
	  /
	  <a href="bibtex/lin2024rethinking.txt">BibTeX</a>
          <p></p>
          <p>
          Propose a new visual prompt paradigm to insert external knowledge, e.g., localized information, into MLLMs addressing the challenge of fine-grained multimodal content correspondence.
          </p>
        </td>
      </tr> -->
		
<!--         <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/DreamPolisher.gif' width="160" height="130">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2403.17237.pdf">
            <span class="papertitle">DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric Diffusion</span>
          </a>
          <br>
          <strong>Yuanze Lin</strong>,
          <a href="https://www.ron-clark.com/">Ronald Clark</a>,
          <a href="https://eng.ox.ac.uk/people/philip-torr/">Philip Torr</a>
          <br>
          <em>Preprint</em>, 2024
          <br>
          <a href="https://arxiv.org/abs/2403.17237">ArXiv</a>
          /
          <a href="https://yuanze-lin.me/DreamPolisher_page/">Project Page</a>
	  /
	  <a href="https://github.com/yuanze-lin/DreamPolisher">Code</a> 
	  /
	  <a href="bibtex/lin2023text.txt">BibTeX</a>
          <p></p>
          <p>
          Present a two-stage Gaussian Splatting based approach that enforces geometric consistency among views, which can generate consistent and realistic 3D objects.
          </p>
        </td>
      </tr> -->
		
        <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/learnable_regions.png' width="160" height="130">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lin_Text-Driven_Image_Editing_via_Learnable_Regions_CVPR_2024_paper.pdf">
            <span class="papertitle">Text-Driven Image Editing via Learnable Regions</span>
          </a>
          <br>
          <strong>Yuanze Lin</strong>,
          <a href="https://wenz116.github.io/">Yi-Wen Chen</a>,
          <a href="https://sites.google.com/site/yihsuantsai/">Yi-Hsuan Tsai</a>,
          <a href="http://www.lujiang.info/">Lu Jiang</a>,
          <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>
          <br>
          <em>CVPR</em>, 2024
          <br>
          <a href="https://arxiv.org/abs/2311.16432">ArXiv</a>
          /
          <a href="https://yuanze-lin.me/LearnableRegions_page/">Project Page</a>
	  /
	  <a href="https://www.youtube.com/watch?v=FpMWRXFraK8">Video</a> 
	  /
	  <a href="https://github.com/yuanze-lin/Learnable_Regions">Code</a> 
	  /
	  <a href="bibtex/lin2023text.txt">BibTeX</a>
          <p></p>
          <p>
          Introduce a region-based editing network that is trained to generate editing regions utilizing a text-driven editing loss with CLIP guidance, our method can edit the given images based on freely provided language descriptions.
          </p>
        </td>
      </tr>
		
        <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/smaug.png' width="160" height="130">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_SMAUG_Sparse_Masked_Autoencoder_for_Efficient_Video-Language_Pre-Training_ICCV_2023_paper.pdf">
            <span class="papertitle">SMAUG: Sparse Masked Autoencoder for Efficient Video-Language Pre-training</span>
          </a>
          <br>
          <strong>Yuanze Lin</strong>,
          <a href="https://weichen582.github.io/">Chen Wei</a>,
          <a href="https://csrhddlam.github.io/">Huiyu Wang</a>,
          <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
          <a href="https://cihangxie.github.io/">Cihang Xie</a>
          <br>
          <em>ICCV</em>, 2023 
          <br>
          <a href="https://arxiv.org/pdf/2211.11446">ArXiv</a>
          /
          <a href="images/poster_smaug.pdf">Poster</a>
          /
          <a href="images/Presentation_iccv2023.pptx">Slides</a>
          /
          <a href="bibtex/lin2023smaug.txt">BibTeX</a>
          <p></p>
          <p>
          Propose an efficient video-language pre-training framework, which enjoys both competitive performances on text-to-video retrieval and video question answering tasks, and much less pre-training costs by 1.9X or more.
          </p>
        </td>
      </tr>
      
      
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/revive.png' width="160" height="130">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/44956951349095f74492a5471128a7e0-Paper-Conference.pdf">
            <span class="papertitle">REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering</span>
          </a>
          <br>
  <strong>Yuanze Lin</strong>, <a href="https://sites.google.com/view/yujia">Yujia Xie</a>, <a href="https://www.dongdongchen.bid/">Dongdong Chen</a>, <a href="https://xycking.wixsite.com/yichongxu">Yichong Xu</a>, <a href="https://scholar.google.com/citations?user=1b2kKWoAAAAJ&hl=zh-CN">Chenguang Zhu</a>, 
  <a href="https://scholar.google.com/citations?user=k9TsUVsAAAAJ&hl=zh-CN">Lu Yuan</a> 
          <br>
          <em>NeurIPS</em>, 2022
          <br>
          <a href="https://arxiv.org/abs/2206.01201">ArXiv</a> / 
          <a href="https://neurips.cc/media/PosterPDFs/NeurIPS%202022/0bf727e907c5fc9d5356f11e4c45d613.png?t=1667706317.0158653">Poster</a> /
          <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/44956951349095f74492a5471128a7e0-Supplemental-Conference.pdf">Supplementary Material</a> /
          <a href="https://openreview.net/forum?id=wwyiEyK-G5D">OpenReview</a> /
          <a href="https://github.com/yuanze-lin/REVIVE">Code</a> /
          <a href="bibtex/lin2022revive.txt">BibTeX</a>
          <p></p>
          <p>Propose a new knowledge-based VQA method REVIVE, which utilizes the explicit information of object regions not only in the knowledge retrieval stage but also in the answering model. It achieves a new state-of-the-art performance on OK-VQA dataset.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/pseudoq.png' width="160" height="130">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_Pseudo-Q_Generating_Pseudo_Language_Queries_for_Visual_Grounding_CVPR_2022_paper.pdf">
            <span class="papertitle">Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding</span>
          </a>
          <br>
	  <a href="https://scholar.google.com/citations?hl=en&user=ULmStp8AAAAJ">Haojun Jiang*</a>,
	  <strong>Yuanze Lin*</strong>,
          <a href="https://scholar.google.com/citations?user=wv3U3tkAAAAJ&hl=en">Dongchen Han</a>,
          <a href="https://scholar.google.com/citations?user=rw6vWdcAAAAJ&hl=zh-CN">Shiji Song</a>,  
          <a href="http://www.gaohuang.net/">Gao Huang</a>
          <br>
          <em>CVPR</em>, 2022
          <br>
          <a href="https://arxiv.org/abs/2203.08481">ArXiv</a> /
          <a href="https://cloud.tsinghua.edu.cn/f/e5f6df930e5d4b21ae27/">Poster</a> /
          <a href="https://github.com/LeapLabTHU/Pseudo-Q">Code</a> /
          <a href="bibtex/jiang2022pseudo.txt">BibTeX</a>
          <p></p>
          <p>
          Present Pseudo-Q to automatically generate pseudo language queries for supervised training, which achieves superior or comparable performance compared to existing weakly-supervised visual grounding methods on five datasets.
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/adafocus2.png' width="160" height="130">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_AdaFocus_V2_End-to-End_Training_of_Spatial_Dynamic_Networks_for_Video_CVPR_2022_paper.pdf">
            <span class="papertitle">AdaFocus V2: End-to-End Training of Spatial Dynamic Networks for Video Recognition</span>
          </a>
          <br>
          <a href="https://scholar.google.com/citations?user=gBP38gcAAAAJ&hl=zh-CN">Yulin Wang*</a>,
          <a href="https://scholar.google.com/citations?user=Q9cLkdcAAAAJ&hl=en">Yang Yue*</a>,
          <strong>Yuanze Lin</strong>,
          <a href="https://github.com/jianghaojun">Haojun Jiang</a>,
          <a href="https://scholar.google.com/citations?user=31eXgMYAAAAJ&hl=en">Zihang Lai</a>,
          <a href="https://scholar.google.com/citations?hl=en&user=9WmuhUcAAAAJ">Victor Kulikov</a>,
          <a href="https://scholar.google.com/citations?hl=en&user=_V4gCO4AAAAJ">Nikita Orlov</a>,
          <a href="https://scholar.google.com/citations?hl=en&user=WBvt5A8AAAAJ">Humphrey Shi</a>,
          <a href="http://www.gaohuang.net/">Gao Huang</a>
          <br>
          <em>CVPR</em>, 2022
          <br>
          <a href="https://arxiv.org/abs/2112.14238">ArXiv</a> /
          <a href="https://github.com/LeapLabTHU/AdaFocusV2">Code</a> /
          <a href="bibtex/wang2022adafocus.txt">BibTeX</a>
          <p></p>
          <p>
          Reformulate AdaFocus as a simple one-stage algorithm by introducing a differentiable interpolation-based patch selection operation and further present an improved training scheme. Extensive experiments on six benchmark datasets demonstrate its effectiveness.
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/mcn.png' width="160" height="130">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Lin_Self-Supervised_Video_Representation_Learning_With_Meta-Contrastive_Network_ICCV_2021_paper.pdf">
            <span class="papertitle">Self-supervised video representation learning with meta-contrastive network</span>
          </a>
          <br>
          <strong>Yuanze Lin</strong>,
          <a href="https://scholar.google.com/citations?user=Ow4R8-EAAAAJ&hl=en">Xun Guo</a>,
          <a href="https://scholar.google.com/citations?user=djk5l-4AAAAJ&hl=en">Yan Lu</a>
          <br>
          <em>ICCV</em>, 2021
          <br>
          <a href="https://arxiv.org/abs/2108.08426">ArXiv</a> /
          <a href="https://arxiv.org/abs/2108.08426">Poster</a> /
          <a href="bibtex/lin2021self.txt">BibTeX</a>
          <p></p>
          <p>
            Propose a Meta-Contrastive Network (MCN), which combines contrastive learning and meta learning for pre-training. For video action recognition and video retrieval tasks, MCN outperforms state-of-the-art approaches on UCF101 and HMDB51 datasets. 
          </p>
        </td>
      </tr>
            
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/eva_gcn.png' width="160" height="130">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/CVPR2021W/AMFG/papers/Xin_EVA-GCN_Head_Pose_Estimation_Based_on_Graph_Convolutional_Networks_CVPRW_2021_paper.pdf">
            <span class="papertitle">EVA-GCN: Head Pose Estimation Based on Graph Convolutional Networks</span>
          </a>
          <br>
          <a href="https://scholar.google.com/citations?hl=zh-CN&user=gZfZZVQAAAAJ">Miao Xin</a>,
          <a href="https://scholar.google.com/citations?hl=zh-CN&user=6aYncPAAAAAJ">Shentong Mo</a>,
          <strong>Yuanze Lin</strong>
          <br>
          <em>CVPR AMFG Workshop</em>, 2021 &nbsp <a href="images/workshop_best_paper.jpg"><font color="red"><strong>(Best Paper Award)</strong></font></a> 
          <br>
          <a href="https://openaccess.thecvf.com/content/CVPR2021W/AMFG/papers/Xin_EVA-GCN_Head_Pose_Estimation_Based_on_Graph_Convolutional_Networks_CVPRW_2021_paper.pdf">Paper</a> /
          <a href="https://github.com/stoneMo/EVA-GCN">Code</a> /
          <a href="bibtex/xin2021eva.txt">BibTeX</a>
          <p></p>
          <p>
            Construct a landmark-connection graph, and propose to leverage the Graph Convolutional Networks (GCN) to model the complex nonlinear mappings between the graph typologies and the head pose angles.
          </p>
        </td>
      </tr>        
      </tbody></table>

	
     <table style="width:100%;border:0px;margin-top:20px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
        <tr>
        <td style="padding-left:20px;padding-right:20px;padding-bottom:30px;width:100%;vertical-align:middle">
        <h2 style="font-family:Impact; font-size:24px; font-weight:bold"><img src="icons/experience2.png", style="width:32px;height:auto;" /> &nbsp;Experiences</h2>
	<hr />
	</td>
	</tr>    
	</tbody></table>
	  
	<table style="width:100%;border:0px;border-spacing:0px;margin-top:-50px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr> <td style="padding-left:12px;width:15%;vertical-align:center;">
	<img src='icons/MicrosoftResearch.png' style="object-fit:scale-down;width:100px;height:100px" /> 
	</td> <td style="padding:0px;width:85%;vertical-align:left;line-height:100%;text-align:left;"> 
	<a href="https://www.microsoft.com/en-us/research/"> <span class="papertitle">GenAI @ Microsoft Redmond</span> 
	</a> <br> <strong>Researcher Intern, Feb 2024 - Nov 2024 </strong> <br> 
	<em>hosted by Dr. <a href="https://www.dongdongchen.bid/"> Dongdong Chen</a>, working on multimodal large language models (MLLMs).</em> 
	</td> </tr> 
	  
        <tr> <td style="padding-left:23px;width:15%;vertical-align:center;margin-top:-5px;border-collapse:separate;"> 
	<img src='icons/uc_merced.png' style="width:85px;height:85px;" /> 
	</td> <td style="padding-left:0px;width:85%;vertical-align:left;line-height:100%;text-align:left;"> 
        <a href="http://vllab.ucmerced.edu/"> <span class="papertitle">Vision and Learning Lab @ UC Merced</span> 
	</a> <br> <strong>Visiting Student, May 2022 - Nov 2023</strong> <br>
        <em>hosted by Prof. <a href="https://scholar.google.com/citations?user=p9-ohHsAAAAJ&hl=en"> Ming-Hsuan Yang</a>, working on text-driven image editing.</em> 
	</td></tr>
		
        <tr> <td style="padding-left:15px;width:15%;vertical-align:center;margin-top:-20px;border-collapse:separate;"> 
	<img src='icons/alibaba2.png' style="width:115px;height:110px;" /> 
	</td> <td style="padding:0px;width:85%;vertical-align:left;line-height:100%;text-align:left;"> 
        <a href="https://www.alibabagroup.com/en-US/"> <span class="papertitle">Alibaba Group</span> 
	</a> <br> <strong>Senior Algorithm Engineer, Feb 2023 - Aug 2023</strong> <br>
        <em>Working on vision-language pre-training, fine-tuning, and the applicability of large language models (LLMs).</em> 
	</td></tr>

        <tr> <td style="padding-left:31px;width:15%;vertical-align:center;margin-top:-10px;border-collapse:separate;"> 
	<img src='icons/jhu2.png' style="width:70px;height:65px;" /> 
	</td> <td style="padding:0px;width:85%;vertical-align:left;line-height:100%;text-align:left;"> 
        <a href="https://ccvl.jhu.edu/"> <span class="papertitle">CCVL @ Johns Hopkins University</span> 
	</a> <br> <strong>Research Assistant, May 2022 - Feb 2023</strong> <br>
        <em>with Prof. <a href="https://scholar.google.com/citations?user=X3vVZPcAAAAJ&hl=en"> Cihang Xie</a> and Prof. <a href="https://scholar.google.com/citations?user=FJ-huxgAAAAJ&hl=en"> Alan Yuille</a>, working on vision-language pre-training based on MAE.</em> 
	</td></tr>

	<tr> <td style="padding-left:12px;width:15%;vertical-align:center;margin-top:10px;margin-bottom:30px;border-collapse:separate;"> <img src='icons/MicrosoftResearch.png' style="object-fit:scale-down;width:100px;height:100px" /> 
	</td> <td style="padding:0px;width:85%;vertical-align:left;line-height:100%;text-align:left;"> 
	<a href="https://www.microsoft.com/en-us/research/project/project-florence-vl/publications/"> 
	<span class="papertitle">Microsoft Redmond</span> </a> <br> <strong>Researcher Intern, Feb 2022 - June 2022</strong> 
	<br> <em>with Dr. <a href="https://scholar.google.com/citations?user=r2FiAE4AAAAJ&hl=en"> Yujia Xie</a>, Dr. <a href="https://scholar.google.com/citations?hl=en&user=sYKpKqEAAAAJ"> Dongdong Chen</a> and Dr. <a href="https://scholar.google.com/citations?user=sYza2XwAAAAJ&hl=en"> Yichong Xu</a>, working on knowledge-based VQA.</em> 
	</td> </tr> 


<!-- 	<tr> <td style="padding-left:25px;width:15%;vertical-align:center;border-collapse:separate;"> <img src='icons/tsinghua.png' style="width:80px;height:80px;" /> 
	</td> <td style="padding:0px;width:85%;vertical-align:left;line-height:100%;text-align:left;"> 
	<a href="https://www.gaohuang.net/"> 
	<span class="papertitle">Tsinghua University</span> </a> <br> <strong>Research Assistant, Sep 2021 - Mar 2022</strong> 
	<br> <em>with Prof. <a href="https://scholar.google.com/citations?user=-P9LwcgAAAAJ&hl=en"> Gao Huang</a>, working on visual grounding and efficient video recognition.</em> 
	</td> </tr> 
 -->
	<tr> <td style="padding-left:14px;width:15%;vertical-align:center;border-collapse:separate;"> <img src='icons/Microsoft_Research_Asia_logo.png' style="object-fit:scale-down;width:100px;height:100px" /> 
	</td> <td style="padding:0px;width:85%;vertical-align:left;line-height:100%;text-align:left;"> 
	<a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/"> 
	<span class="papertitle">Microsoft Research Asia</span> </a> <br> <strong>Researcher Intern, Dec 2020 - Sep 2021</strong> 
	<br> <em>with Dr. <a href="https://scholar.google.com/citations?user=Ow4R8-EAAAAJ&hl=en"> Xun Guo</a> and Dr. <a href="https://scholar.google.com/citations?user=djk5l-4AAAAJ&hl=en"> Yan Lu</a>, working on self-supervised learning and transformers for video tasks.</em> 
	</td> </tr> 
		

	<tr> <td style="padding-left:26px;width:15%;vertical-align:center;bmborder-collapse:separate;"> <img src='icons/tencent_ai2.png' style="width:80px;height:80px;" /> 
	</td> <td style="padding:0px;width:85%;vertical-align:left;line-height:100%;text-align:left;"> 
	<a href="https://ai.tencent.com/ailab/en/about"> 
	<span class="papertitle">Tencent AI Lab</span> </a> <br> <strong>Researcher Intern, Sep 2020 - Dec 2020</strong> 
	<br> <em>with Dr. <a href="https://scholar.google.com/citations?hl=en&user=wTJ83eEAAAAJ"> Haozhi Huang</a>, working on text-driven editing of videos based on meta learning.</em> 
	</td> </tr>

	</td>
	</tr>    
	</tbody></table>
		
         <table style="width:100%;padding-top:40px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2 style="font-family:Impact; font-size:24px; font-weight:bold"><img src="icons/activity.png", style="width:32px;height:auto;" /> &nbsp; Professional Services</h2>
		<hr />
		<p style="line-height:0.8em;font-size:14px;"> <em> Program Comittee: AAAI 2025 </em> </p> 
		<p style="line-height:0.8em;font-size:14px;"> <em> Journal Reviewer: IJCV 2025 </em> </p> 
		<p style="line-height:0.8em;font-size:14px;"> <em> Conference Reviewer: ICLR 2025, AISTATS 2025, CVPR 2025, ICML 2025, ICCV 2025, NeurIPS 2025</em> </p>    
                <p style="line-height:0.8em;font-size:14px;"> <em> Conference Reviewer: ICRA 2024, CVPR 2024, ECCV 2024, NeurIPS 2024</em> </p>
		<p style="line-height:0.8em;font-size:14px;"> <em> Conference Reviewer: ICLR 2023, CVPR 2023, ICCV 2023, NeurIPS 2023 </em> </p>
		<p style="line-height:0.8em;font-size:14px;"> <em> Conference Reviewer: CVPR 2022 </em> </p>
              </td>
            </tr>
          </tbody></table>
	  
<!--          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
	      <td style="padding:20px;width:100%;vertical-align:middle">
		<a href="https://clustrmaps.com/site/1bwxo" title="Visit tracker"><img src="//clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=tt&d=LpeUFK6E0UqKyMayHxJ376iYqiMie7Njj0rKOZhMZhE" /></a>
              </td>
            </tr>
          </tbody></table> -->
	  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px">
                <br>
                <p style="text-align:right;font-size:small;">
                  No web trackers, feel free to see this website&#12288;&#12288;&#12288;&#12288;&#12288;&#12288; Last Update: 04/2025  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  <a href="https://jonbarron.info/">Template</a>
                </p>
              </td>
            </tr>
          </tbody></table>
          
  </body>
</html>
