<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yuanze Lin</title>

    <meta name="author" content="Yuanze Lin">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/oxford1.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yuanze Lin
                </p>
                <p>I am a first-year DPhil student at Computer Science department of <a href="https://www.ox.ac.uk/">University of Oxford</a>, where I work with Professor <a href="https://scholar.google.com/citations?hl=en&user=0bzqo0YAAAAJ">Ronald Clark</a> and Professor <a href="https://scholar.google.com/citations?hl=en&user=185g9ckAAAAJ">Niki Trigoni</a>.
                <p>
                  Before going to Oxford, I spent great time at <a href="https://www.microsoft.com/en-us/research/project/project-florence-vl/publications/">Microsoft Redmond</a>, <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">MSRA</a>, <a href="https://ccvl.jhu.edu/">CCVL @ Johns Hopkins University</a>, <a href="https://www.tsinghua.edu.cn/en/"> Tsinghua University</a>, <a href="https://ai.tencent.com/ailab/en/about/"> Tencent AI Lab</a>, etc. I'm so lucky to work with <a href="https://www.microsoft.com/en-us/research/people/xunguo/"> Xun Guo</a> and 
                  <a href="https://www.microsoft.com/en-us/research/people/yanlu/"> Yan Lu</a> at MSRA,  <a href="http://www.gaohuang.net/"> Gao Huang</a> at Tsinghua University, <a href="https://sites.google.com/view/yujia"> Yujia Xie</a>, <a href="https://www.dongdongchen.bid/"> Dongdong Chen</a> and <a href="https://xycking.wixsite.com/yichongxu"> Yichong Xu</a> at Microsoft Redmond, <a href="https://cihangxie.github.io/"> Cihang Xie</a> and <a href="https://www.cs.jhu.edu/~ayuille/"> Alan Yuille</a> at Johns Hopkins University.
                </p>
                <p>
                  My research interest lies in computer vision and machine learning, with an emphasis on self-supervised learning, multimodal learning, 3D vision, and large language models. Feel free to email me for research discussion : )
                </p>
                <p>
                 <font color='#ff0000'><em> Currently, I'm open to 2024 winter and summer research internships, especially about multimodal learning and 3D vision! </em></font>
                </p>
                <p style="text-align:center">
                  <a href="mailto:yuanze.lin@cs.ox.ac.uk">yuanze.lin [at] cs.ox.ac.uk</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=0WFC2w0AAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/yuanze-lin/">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/yuanze-lin-720543139/">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/yuanze.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/yuanze_circle.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>News</h2>
                <br>
                <p style="line-height:0.8em;"> [10/2023]&nbsp;&nbsp; Started my PhD journey at University of Oxford. </p>
                <p style="line-height:0.8em;"> [07/2023]&nbsp;&nbsp; SMAUG accepted to ICCV 2023. </p>
                <p style="line-height:0.8em;"> [09/2022]&nbsp;&nbsp; REVIVE accepted to NeurIPS 2022. </p>
                <p style="line-height:0.8em;"> [03/2022]&nbsp;&nbsp; Pseudo-Q and AdaFocus V2 accepted to CVPR 2022. </p>
                <p style="line-height:0.8em;"> [07/2021]&nbsp;&nbsp; MCN accepted to ICCV 2021. </p>
                <p style="line-height:0.8em;"> [06/2021]&nbsp;&nbsp; EVA-GCN accepted to CVPR 2021 AMFG Workshop and won Best Paper Award. </p>
              </td>
            <tr>
          </tbody></table>
	  
            
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm particularly interested in computer vision, especially about how to efficiently utilize images and texts for pre-training or solving various multimodal tasks.  Selected papers are <span class="highlight">highlighted</span> and <b>*</b> indicates <b>equal contribution</b>.
                </p>
              </td>
            </tr>
          </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/smaug.png' width="160">
        </td>
        
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_SMAUG_Sparse_Masked_Autoencoder_for_Efficient_Video-Language_Pre-Training_ICCV_2023_paper.pdf">
            <span class="papertitle">SMAUG: Sparse Masked Autoencoder for Efficient Video-Language Pre-training</span>
          </a>
          <br>
          <strong>Yuanze Lin</strong>,
          <a href="https://weichen582.github.io/">Chen Wei</a>,
          <a href="https://csrhddlam.github.io/">Huiyu Wang</a>,
          <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
          <a href="https://cihangxie.github.io/">Cihang Xie</a>
          <br>
          <em>ICCV</em>, 2023 
          <br>
          <a href="https://arxiv.org/pdf/2211.11446">ArXiv</a>
          /
          <a href="images/poster_smaug.pdf">Poster</a>
          /
          <a href="images/Presentation_iccv2023.pptx">Slides</a>
          /
          <a href="images/ICCV2023_SMAUG_Video.mp4">Video</a>
          /
          <a href="bibtex/lin2023smaug.txt">Bibtex</a>
          <p></p>
          <p>
          Propose an efficient multimodal pre-training framework, which enjoys both competitive performances on text-to-video retrieval and video question answering tasks, and much less pre-training costs by 1.9X or more.
          </p>
        </td>
      </tr>
      
      
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/revive.png' width="160">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/44956951349095f74492a5471128a7e0-Paper-Conference.pdf">
            <span class="papertitle">REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering</span>
          </a>
          <br>
  <strong>Yuanze Lin</strong>, <a href="https://sites.google.com/view/yujia">Yujia Xie</a>, <a href="https://www.dongdongchen.bid/">Dongdong Chen</a>, <a href="https://xycking.wixsite.com/yichongxu">Yichong Xu</a>, <a href="https://scholar.google.com/citations?user=1b2kKWoAAAAJ&hl=zh-CN">Chenguang Zhu</a>, 
  <a href="https://scholar.google.com/citations?user=k9TsUVsAAAAJ&hl=zh-CN">Lu Yuan</a> 
          <br>
          <em>NeurIPS</em>, 2022
          <br>
          <a href="https://arxiv.org/abs/2206.01201">ArXiv</a> / 
          <a href="https://neurips.cc/media/PosterPDFs/NeurIPS%202022/0bf727e907c5fc9d5356f11e4c45d613.png?t=1667706317.0158653">Poster</a> /
          <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/44956951349095f74492a5471128a7e0-Supplemental-Conference.pdf">Supplementary Material</a> /
          <a href="https://openreview.net/forum?id=wwyiEyK-G5D">OpenReview</a> /
          <a href="https://neurips.cc/virtual/2022/poster/54966">Video</a> /
          <a href="https://github.com/yuanze-lin/REVIVE">Code</a> /
          <a href="bibtex/lin2022revive.txt">Bibtex</a>
          <p></p>
          <p>Propose a new knowledge-based VQA method REVIVE, which tries to utilize the explicit information of object regions not only in the knowledge retrieval stage but also in the answering model. We perform extensive experiments on the standard OK-VQA dataset and achieve new state-of-the-art performance.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/pseudoq.png' width="160">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_Pseudo-Q_Generating_Pseudo_Language_Queries_for_Visual_Grounding_CVPR_2022_paper.pdf">
            <span class="papertitle">Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding</span>
          </a>
          <br>
          <a href="https://github.com/jianghaojun">Haojun Jiang*</a>,
          <strong>Yuanze Lin*</strong>,
          <a href="https://scholar.google.com/citations?user=wv3U3tkAAAAJ&hl=en">Dongchen Han</a>,
          <a href="https://scholar.google.com/citations?user=rw6vWdcAAAAJ&hl=zh-CN">Shiji Song</a>,  
          <a href="http://www.gaohuang.net/">Gao Huang</a>
          <br>
          <em>CVPR</em>, 2022
          <br>
          <a href="https://arxiv.org/abs/2203.08481">ArXiv</a> /
          <a href="https://cloud.tsinghua.edu.cn/f/e5f6df930e5d4b21ae27/">Poster</a> /
          <a href="https://cloud.tsinghua.edu.cn/f/d655d6e2a6b246b4bb4f/">Video</a> /
          <a href="https://github.com/LeapLabTHU/Pseudo-Q">Code</a> /
          <a href="bibtex/jiang2022pseudo.txt">Bibtex</a>
          <p></p>
          <p>
          Present a novel method, named Pseudo-Q, to automatically generate pseudo language queries for supervised training, t achieves superior or comparable performance compared to state-of-the-art weakly-supervised visual grounding methods on all the five datasets we have experimented.
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/adafocus2.png' width="160">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_AdaFocus_V2_End-to-End_Training_of_Spatial_Dynamic_Networks_for_Video_CVPR_2022_paper.pdf">
            <span class="papertitle">AdaFocus V2: End-to-End Training of Spatial Dynamic Networks for Video Recognition</span>
          </a>
          <br>
          <a href="https://scholar.google.com/citations?user=gBP38gcAAAAJ&hl=zh-CN">Yulin Wang*</a>,
          <a href="https://scholar.google.com/citations?user=Q9cLkdcAAAAJ&hl=en">Yang Yue*</a>,
          <strong>Yuanze Lin</strong>,
          <a href="https://github.com/jianghaojun">Haojun Jiang</a>,
          <a href="https://scholar.google.com/citations?user=31eXgMYAAAAJ&hl=en">Zihang Lai</a>,
          <a href="https://scholar.google.com/citations?hl=en&user=9WmuhUcAAAAJ">Victor Kulikov</a>,
          <a href="https://scholar.google.com/citations?hl=en&user=_V4gCO4AAAAJ">Nikita Orlov</a>,
          <a href="https://scholar.google.com/citations?hl=en&user=WBvt5A8AAAAJ">Humphrey Shi</a>,
          <a href="http://www.gaohuang.net/">Gao Huang</a>
          <br>
          <em>CVPR</em>, 2022
          <br>
          <a href="https://arxiv.org/abs/2112.14238">ArXiv</a> /
          <a href="https://github.com/LeapLabTHU/AdaFocusV2">Code</a> /
          <a href="bibtex/wang2022adafocus.txt">Bibtex</a>
          <p></p>
          <p>
          Reformulate the training of AdaFocus as a simple one-stage algorithm by introducing a differentiable interpolation-based patch selection operation and further present an improved training scheme. Extensive experiments on six benchmark datasets demonstrate its effectiveness.
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/mcn.png' width="160">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Lin_Self-Supervised_Video_Representation_Learning_With_Meta-Contrastive_Network_ICCV_2021_paper.pdf">
            <span class="papertitle">Self-supervised video representation learning with meta-contrastive network</span>
          </a>
          <br>
          <strong>Yuanze Lin</strong>,
          <a href="https://scholar.google.com/citations?user=Ow4R8-EAAAAJ&hl=en">Xun Guo</a>,
          <a href="https://scholar.google.com/citations?user=djk5l-4AAAAJ&hl=en">Yan Lu</a>
          <br>
          <em>ICCV</em>, 2021
          <br>
          <a href="https://arxiv.org/abs/2108.08426">ArXiv</a> /
          <a href="https://arxiv.org/abs/2108.08426">Poster</a> /
          <a href="bibtex/lin2021self.txt">Bibtex</a>
          <p></p>
          <p>
            Propose a Meta-Contrastive Network (MCN), which combines contrastive learning and meta learning, to enhance the learning ability of existing self-supervised approaches, For two downstream tasks, i.e., video action recognition and video retrieval, MCN outperforms state-of-the-art approaches on UCF101 and HMDB51 datasets. 
          </p>
        </td>
      </tr>
            
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <img src='images/eva_gcn.png' width="160">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/CVPR2021W/AMFG/papers/Xin_EVA-GCN_Head_Pose_Estimation_Based_on_Graph_Convolutional_Networks_CVPRW_2021_paper.pdf">
            <span class="papertitle">EVA-GCN: Head Pose Estimation Based on Graph Convolutional Networks</span>
          </a>
          <br>
          <a href="https://scholar.google.com/citations?hl=zh-CN&user=gZfZZVQAAAAJ">Miao Xin</a>,
          <a href="https://scholar.google.com/citations?hl=zh-CN&user=6aYncPAAAAAJ">Shentong Mo</a>,
          <strong>Yuanze Lin</strong>
          <br>
          <em>CVPR AMFG Workshop</em>, 2021 &nbsp <a href="images/workshop_best_paper.jpg"><font color="red"><strong>(Best Paper Award)</strong></font></a> 
          <br>
          <a href="https://openaccess.thecvf.com/content/CVPR2021W/AMFG/papers/Xin_EVA-GCN_Head_Pose_Estimation_Based_on_Graph_Convolutional_Networks_CVPRW_2021_paper.pdf">Paper</a> /
          <a href="https://github.com/stoneMo/EVA-GCN">Code</a> /
          <a href="bibtex/xin2021eva.txt">Bibtex</a>
          <p></p>
          <p>
            Construct a landmark-connection graph, and propose to leverage the Graph Convolutional Networks (GCN) to model the complex nonlinear mappings between the graph typologies and the head pose angles.
          </p>
        </td>
      </tr>        
          
         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Experiences</h2>
		 </td>
               </tr>
         </tbody></table>	      

        <tr>
       	<td style="padding:0px;width:20%;vertical-align:center;horizontal-align:center;">
        	<img src='icons/alibaba2.png' width="120">
	</td>
	<td style="padding:0px;width:75%;vertical-align:left;line-height:150%;text-align:left;">
          <a href="https://www.alibabagroup.com/en-US/">
            <span class="papertitle">Alibaba Group</span>
          </a>
	  <br>
          <strong>Senior Algorithm Engineer</strong>
          <br>
	  <em>Feb 2023 - Aug 2023</em>
        </td>
	</tr>

	<tr>
	<td style="padding:10px;width:20%;vertical-align:center;horizontal-align:center;">
       		<img src='icons/jhu2.png' width="70"> 
	</td>
	<td style="padding:0px;width:75%;vertical-align:left;line-height:150%;text-align:left;">
          <a href="https://ccvl.jhu.edu/">
            <span class="papertitle">Johns Hopkins University (CCVL)</span>
          </a>
	  <br>
          <strong>Research Assistant</strong>
          <br>
	  <em>May 2022 - Feb 2023</em>	
        </td>
	</tr>
		
        <tr>
       	<td style="padding:0px;width:20%;vertical-align:center;horizontal-align:center;">
        	<img src='icons/microsoft2.jpg' width="110">
	</td>
        <td style="padding:0px;width:75%;vertical-align:left;line-height:150%;text-align:left;">
          <a href="https://www.microsoft.com/en-us/research/project/project-florence-vl/publications/">
            <span class="papertitle">Microsoft Redmond</span>
          </a>
	  <br>
          <strong>Researcher Intern</strong>
          <br>
          <em>Feb 2022 - June 2022</em>
        </td>
	</tr>

	<tr>
       	<td style="padding:10px;width:20%;vertical-align:center;horizontal-align:center;">
        	<img src='icons/tsinghua.png' width="90">
	</td>
        <td style="padding:0px;width:75%;vertical-align:left;line-height:150%;text-align:left;">
          <a href="https://www.gaohuang.net/">
            <span class="papertitle">Tsinghua University</span>
          </a>
	  <br>
          <strong>Research Assistant</strong>
          <br>
          <em>Sep 2021 - Mar 2022</em>
        </td>
	</tr>
		
	<tr>
       	<td style="padding:0px;width:20%;vertical-align:center;horizontal-align:center;">
        	<img src='icons/microsoft2.jpg' width=110">
	</td>
        <td style="padding:0px;width:75%;vertical-align:left;line-height:150%;text-align:left;">
          <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">
            <span class="papertitle">Microsoft Research Asia</span>
          </a>
	  <br>
          <strong>Researcher Intern</strong>
          <br>
          <em>Dec 2020 - Sep 2021</em>
        </td>
	</tr>

	<tr>
       	<td style="padding:10px;width:20%;vertical-align:center;horizontal-align:center;">
        	<img src='icons/tencent_ai2.png' width="80">
	</td>
        <td style="padding:0px;width:75%;vertical-align:left;line-height:150%;text-align:left;">
          <a href="https://ai.tencent.com/ailab/en/about">
            <span class="papertitle">Tencent AI Lab</span>
          </a>
	  <br>
          <strong>Researcher Intern</strong>
          <br>
          <em>Sep 2020 - Dec 2020</em>
        </td>
	</tr>
	
         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Professional Activities</h2>
                <br>
		<ul>
                <p style="line-height:0.8em;"> <em> Conference Reviewer: CVPR 2022, ICLR 2023, CVPR 2023, ICCV 2023, NeurIPS 2023 </em>
		<ul> </p>
              </td>
            </tr>
          </tbody></table>
		  
		  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Last Update: 10/2023  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  <a href="https://jonbarron.info/">Template</a>
                </p>
              </td>
            </tr>
          </tbody></table>
          
  </body>
</html>
