<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yuanze Lin</title>

    <meta name="author" content="Yuanze Lin">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yuanze Lin
                </p>
                <p>I am a Computer Science DPhil student at <a href="https://www.ox.ac.uk/">University of Oxford</a>, advised by <a href="https://scholar.google.com/citations?hl=en&user=0bzqo0YAAAAJ">Ronald Clark</a> and <a href="https://scholar.google.com/citations?hl=en&user=185g9ckAAAAJ">Niki Trigoni</a>, my research interests include self-supervised learning, multimodal learning and large language models.
                <p>
                  Before going to Oxford, I've done research at <a href="https://www.microsoft.com/en-us/research/project/project-florence-vl/publications/">Microsoft Redmond</a>, <a href="https://www.msra.cn/">MSRA</a>, <a href="https://www.jhu.edu/">Johns Hopkins University</a>, <a href="https://www.tsinghua.edu.cn/en/"> Tsinghua University</a>, etc. I'm so lucky to work with <a href="https://www.microsoft.com/en-us/research/people/xunguo/"> Xun Guo</a> and 
                  <a href="https://www.microsoft.com/en-us/research/people/yanlu/"> Yan Lu</a> at MSRA,  <a href="http://www.gaohuang.net/"> Gao Huang</a> at Tsinghua University, <a href="https://sites.google.com/view/yujia"> Yujia Xie</a>, <a href="https://www.dongdongchen.bid/"> Dongdong Chen</a> and <a href="https://xycking.wixsite.com/yichongxu">Yichong Xu</a> at Microsoft Redmond, <a href="https://cihangxie.github.io/"> Cihang Xie</a> and <a href="https://www.cs.jhu.edu/~ayuille/"> Alan Yuille</a> at Johns Hopkins University.
                </p>
                <p style="text-align:center">
                  <a href="mailto:yuanze.lin@cs.ox.ac.uk">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=0WFC2w0AAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://openreview.net/profile?id=~Yuanze_Lin1">OpenReview</a> &nbsp;/&nbsp;
                  <a href="https://github.com/yuanze-lin/">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/yuanze-lin-720543139/">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/yuanze.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/yuanze_circle.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm particularly interested in computer vision, especially about how to efficiently utilize images and texts for pre-training or solving various multimodal tasks.  Selected papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
    
      <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()"  bgcolor="#ffffd0">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/zipnerf.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/zipnerf.jpg' width="160">
          </div>
          <script type="text/javascript">
            function zipnerf_start() {
              document.getElementById('zipnerf_image').style.opacity = "1";
            }

            function zipnerf_stop() {
              document.getElementById('zipnerf_image').style.opacity = "0";
            }
            zipnerf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2211.11446">
            <span class="papertitle">SMAUG: Sparse Masked Autoencoder for Efficient Video-Language Pre-training</span>
          </a>
          <br>
          <strong>Yuanze Lin</strong>,
          <a href="https://weichen582.github.io/">Chen Wei</a>,
          <a href="https://csrhddlam.github.io/">Huiyu Wang</a>,
          <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
          <a href="https://cihangxie.github.io/">Cihang Xie</a>,
          <br>
          <em>ICCV</em>, 2023 
          <br>
          <a href="https://arxiv.org/pdf/2211.11446">ArXiv</a>
          /
          <a href="https://arxiv.org/pdf/2211.11446">Poster</a>
          /
          <a href="https://arxiv.org/pdf/2211.11446">Video</a>
          <p></p>
          <p>
          Propose an efficient multimodal pre-training framework, which enjoys both competitive performances on text-to-video retrieval and video question answering tasks, and much less pre-training costs by 1.9X or more.
          </p>
        </td>
      </tr>
      
      
      <tr onmouseout="db3d_stop()" onmouseover="db3d_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='db3d_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/owl.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/owl.png' width="160">
          </div>
          <script type="text/javascript">
            function db3d_start() {
              document.getElementById('db3d_image').style.opacity = "1";
            }

            function db3d_stop() {
              document.getElementById('db3d_image').style.opacity = "0";
            }
            db3d_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/44956951349095f74492a5471128a7e0-Paper-Conference.pdf">
            <span class="papertitle">REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering</span>
          </a>
          <br>
  <strong>Yuanze Lin</strong>, <a href="https://sites.google.com/view/yujia">Yujia Xie</a>, <a href="https://www.dongdongchen.bid/">Dongdong Chen</a>, <a href="https://xycking.wixsite.com/yichongxu">Yichong Xu</a>, <a href="https://scholar.google.com/citations?user=1b2kKWoAAAAJ&hl=zh-CN">Chenguang Zhu</a>, 
  <a href="https://scholar.google.com/citations?user=k9TsUVsAAAAJ&hl=zh-CN">Lu Yuan</a>, 
          
          <br>
          <em>NeurIPS</em>, 2022
          <br>
          <a href="https://arxiv.org/abs/2206.01201">ArXiv</a> / 
          <a href="https://neurips.cc/media/PosterPDFs/NeurIPS%202022/0bf727e907c5fc9d5356f11e4c45d613.png?t=1667706317.0158653">Poster</a> /
          <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/44956951349095f74492a5471128a7e0-Supplemental-Conference.pdf">Supplementary Material</a> /
          <a href="https://openreview.net/forum?id=wwyiEyK-G5D">OpenReview</a> /
          <a href="https://github.com/yuanze-lin/REVIVE">Code</a> 
          <p></p>
          <p>Propose a new knowledge-based VQA method REVIVE, which tries to utilize the explicit information of object regions not only in the knowledge retrieval stage but also in the answering model. We perform extensive experiments on the standard OK-VQA dataset and achieve new state-of-the-art performance, i.e., 58.0% accuracy, surpassing previous state-of-the-art method by a large margin (+3.6%).</p>
        </td>
      </tr>

      

      <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='bakedsdf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/bakedsdf_after.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/bakedsdf_before.jpg' width="160">
          </div>
          <script type="text/javascript">
            function bakedsdf_start() {
              document.getElementById('bakedsdf_image').style.opacity = "1";
            }

            function bakedsdf_stop() {
              document.getElementById('bakedsdf_image').style.opacity = "0";
            }
            bakedsdf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_Pseudo-Q_Generating_Pseudo_Language_Queries_for_Visual_Grounding_CVPR_2022_paper.pdf">
            <span class="papertitle">Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding</span>
          </a>
          <br>
          <a href="https://github.com/jianghaojun">Haojun Jiang*</a>,
          <strong>Yuanze Lin*</strong>,
          <a href="https://scholar.google.com/citations?user=wv3U3tkAAAAJ&hl=en">Dongchen Han,</a>,
          <a href="https://scholar.google.com/citations?user=rw6vWdcAAAAJ&hl=zh-CN">Shiji Song</a>,  
          <a href="http://www.gaohuang.net/">Gao Huang</a>, 
          <br>
          <em>CVPR</em>, 2022
          <br>
          <a href="https://arxiv.org/abs/2203.08481">ArXiv</a> /
          <a href="https://cloud.tsinghua.edu.cn/f/e5f6df930e5d4b21ae27/">Poster</a> /
          <a href="https://cloud.tsinghua.edu.cn/f/d655d6e2a6b246b4bb4f/">Video</a> /
          <a href="https://github.com/LeapLabTHU/Pseudo-Q">Code</a> 
          <p></p>
          <p>
          Present a novel method, named Pseudo-Q, to automatically generate pseudo language queries for supervised training, t achieves superior or comparable performance compared to state-of-the-art weakly-supervised visual grounding methods on all the five datasets we have experimented.
          </p>
        </td>
      </tr>

      <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='bakedsdf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/bakedsdf_after.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/bakedsdf_before.jpg' width="160">
          </div>
          <script type="text/javascript">
            function bakedsdf_start() {
              document.getElementById('bakedsdf_image').style.opacity = "1";
            }

            function bakedsdf_stop() {
              document.getElementById('bakedsdf_image').style.opacity = "0";
            }
            bakedsdf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_AdaFocus_V2_End-to-End_Training_of_Spatial_Dynamic_Networks_for_Video_CVPR_2022_paper.pdf">
            <span class="papertitle">AdaFocus V2: End-to-End Training of Spatial Dynamic Networks for Video Recognition</span>
          </a>
          <br>
          <a href="https://scholar.google.com/citations?user=gBP38gcAAAAJ&hl=zh-CN">Yulin Wang*</a>,
          <a href="https://scholar.google.com/citations?user=Q9cLkdcAAAAJ&hl=en">Yang Yue*</a>,
          <strong>Yuanze Lin*</strong>,
          <a href="https://github.com/jianghaojun">Haojun Jiang</a>,
          <a href="https://scholar.google.com/citations?user=31eXgMYAAAAJ&hl=en">Zihang Lai</a>,
          <a href="https://scholar.google.com/citations?hl=en&user=9WmuhUcAAAAJ">Victor Kulikov</a>,
          <a href="https://scholar.google.com/citations?hl=en&user=_V4gCO4AAAAJ">Nikita Orlov</a>,
          <a href="https://scholar.google.com/citations?hl=en&user=WBvt5A8AAAAJ">Humphrey Shi</a>,
          <a href="http://www.gaohuang.net/">Gao Huang</a>,
          <br>
          <em>CVPR</em>, 2022
          <br>
          <a href="https://arxiv.org/abs/2112.14238">ArXiv</a> /
          <a href="https://github.com/LeapLabTHU/AdaFocusV2">Code</a> 
          <p></p>
          <p>
          Reformulate the training of AdaFocus as a simple one-stage algorithm by introducing a differentiable interpolation-based patch selection operation and further present an improved training scheme. Extensive experiments on six benchmark datasets (i.e., ActivityNet, FCVID, Mini-Kinetics, Something-Something V1&V2, and Jester) demonstrate its effectiveness.
          </p>
        </td>
      </tr>

      <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='bakedsdf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/bakedsdf_after.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/bakedsdf_before.jpg' width="160">
          </div>
          <script type="text/javascript">
            function bakedsdf_start() {
              document.getElementById('bakedsdf_image').style.opacity = "1";
            }

            function bakedsdf_stop() {
              document.getElementById('bakedsdf_image').style.opacity = "0";
            }
            bakedsdf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Lin_Self-Supervised_Video_Representation_Learning_With_Meta-Contrastive_Network_ICCV_2021_paper.pdf">
            <span class="papertitle">Self-supervised video representation learning with meta-contrastive network</span>
          </a>
          <br>
          <strong>Yuanze Lin*</strong>,
          <a href="https://scholar.google.com/citations?user=Ow4R8-EAAAAJ&hl=en">Xun Guo,</a>,
          <a href="https://scholar.google.com/citations?user=djk5l-4AAAAJ&hl=en">Yan Lu</a>, 
          <br>
          <em>ICCV</em>, 2021
          <br>
          <a href="https://arxiv.org/abs/2108.08426">ArXiv</a> 
          <p></p>
          <p>
            Propose a Meta-Contrastive Network (MCN), which combines the contrastive learning and meta learning, to enhance the learning ability of existing self-supervised approaches, For two downstream tasks, i.e., video action recognition and video retrieval, MCN outperforms state-of-theart approaches on UCF101 and HMDB51 datasets. To be more specific, with R(2+1)D backbone, MCN achieves Top1 accuracies of 84.8% and 54.5% for video action recognition, as well as 52.5% and 23.7% for video retrieval.
          </p>
        </td>
      </tr>

            
      <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='bakedsdf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/bakedsdf_after.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/bakedsdf_before.jpg' width="160">
          </div>
          <script type="text/javascript">
            function bakedsdf_start() {
              document.getElementById('bakedsdf_image').style.opacity = "1";
            }

            function bakedsdf_stop() {
              document.getElementById('bakedsdf_image').style.opacity = "0";
            }
            bakedsdf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/CVPR2021W/AMFG/papers/Xin_EVA-GCN_Head_Pose_Estimation_Based_on_Graph_Convolutional_Networks_CVPRW_2021_paper.pdf">
            <span class="papertitle">EVA-GCN: Head Pose Estimation Based on Graph Convolutional Networks</span>
          </a>
          <br>
          <a href="https://scholar.google.com/citations?hl=zh-CN&user=gZfZZVQAAAAJ">Miao Xin,</a>,
          <a href="https://scholar.google.com/citations?hl=zh-CN&user=6aYncPAAAAAJ">Shentong Mo</a>, 
          <strong>Yuanze Lin*</strong>,
          <br>
          <em>CVPR AMFG Workshop</em>, 2021 &nbsp <font color="red"><strong>(Best Paper Award)</strong></font>
          <br>
          <a href="https://arxiv.org/abs/2108.08426">ArXiv</a> 
          <a href="https://github.com/stoneMo/EVA-GCN">Code</a> 
          <p></p>
          <p>
            Construct a landmark-connection graph, and propose to leverage the Graph Convolutional Networks (GCN) to model the complex nonlinear mappings between the graph typologies and the head pose angles.
          </p>
        </td>
      </tr>
            
          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Experience</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
              <td width="75%" valign="center">
                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
          <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                <br>
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member, CVPR 2021</a>
                <br>
                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                <br>
                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/cs188.jpg" alt="cs188">
              </td>
              <td width="75%" valign="center">
                <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
                <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
                <br>
                <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
              </td>
            </tr>
            

            <tr>
              <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                <h2>Basically <br> Blog Posts</h2>
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr>
            
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
